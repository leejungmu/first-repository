{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package 'keras' successfully unpacked and MD5 sums checked\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\JMLee\\AppData\\Local\\Temp\\Rtmpw5OMqt\\downloaded_packages\n"
     ]
    }
   ],
   "source": [
    "install.packages(\"keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###DQN Model\n",
    "model <- keras_model_sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Layer (type)                        Output Shape                    Param #     \n",
      "================================================================================\n",
      "dense (Dense)                       (None, 256)                     25856       \n",
      "________________________________________________________________________________\n",
      "dropout (Dropout)                   (None, 256)                     0           \n",
      "________________________________________________________________________________\n",
      "dense_1 (Dense)                     (None, 128)                     32896       \n",
      "________________________________________________________________________________\n",
      "dropout_1 (Dropout)                 (None, 128)                     0           \n",
      "________________________________________________________________________________\n",
      "dense_2 (Dense)                     (None, 4)                       516         \n",
      "================================================================================\n",
      "Total params: 59,268\n",
      "Trainable params: 59,268\n",
      "Non-trainable params: 0\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model %>% \n",
    "  layer_dense(units = 256, activation = 'relu', input_shape = c(100)) %>% \n",
    "  layer_dropout(rate = 0.4) %>% \n",
    "  layer_dense(units = 128, activation = 'relu') %>%\n",
    "  layer_dropout(rate = 0.3) %>%\n",
    "  layer_dense(units = 4, activation = 'linear')\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model %>% compile(\n",
    "   loss = 'mean_squared_error',\n",
    "   optimizer = optimizer_rmsprop()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Target Network Model\n",
    "target_qn <- keras_model_sequential()\n",
    "target_qn %>%\n",
    "   layer_dense(units = 256, activation = 'relu', input_shape = c(100)) %>%\n",
    "   layer_dropout(rate = 0.4) %>%\n",
    "   layer_dense(units = 128, activation = 'relu') %>%\n",
    "   layer_dropout(rate = 0.3) %>%\n",
    "   layer_dense(units = 4, activation = 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training & Evaluation -----------------------------------------------------\n",
    "\n",
    "freeze_weights(target_qn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord <- function(state){\n",
    "    re_index<-which(state==1)\n",
    "    xx<-ceiling(re_index/ 10)\n",
    "    yy<-re_index %% 10\n",
    "    yy<-ifelse(yy == 0, 10, yy)\n",
    "    c(xx, yy)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action function\n",
    "\n",
    "move<-function(x,action){\n",
    "    if(action == \"left\"){\n",
    "        x\n",
    "    }else{\n",
    "        x[2]<-x[2]-1\n",
    "        x\n",
    "  }\n",
    "      if(action == \"right\"){\n",
    "    if(x[2]+1>ncol(stm)){\n",
    "      x\n",
    "    }else{\n",
    "      x[2]<-x[2]+1\n",
    "      x\n",
    "    }\n",
    "  }\n",
    "  if(action == \"up\"){\n",
    "    if(x[1]-1<1){\n",
    "      x\n",
    "    }else{\n",
    "      x[1]<-x[1]-1\n",
    "      x\n",
    "    }\n",
    "  }\n",
    "  if(action == \"down\"){\n",
    "    if(x[1]+1>nrow(stm)){\n",
    "      x\n",
    "    }else{\n",
    "      x[1]<-x[1]+1\n",
    "      x\n",
    "    }\n",
    "  }\n",
    "  x\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_where<-function(index){ \n",
    "  zero<-rep(0,100)\n",
    "  zero[index]<-1\n",
    "  zero  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in if (re_index == 100) {: 인자의 길이가 0입니다\n",
     "output_type": "error",
     "traceback": [
      "Error in if (re_index == 100) {: 인자의 길이가 0입니다\nTraceback:\n",
      "1. return_reward(next_state, current_state)"
     ]
    }
   ],
   "source": [
    "### state matrix\n",
    "stm<-matrix(1:100,ncol=10,nrow=10,byrow=T)\n",
    "\n",
    "return_reward<-function(state,current_state){\n",
    "  re_index<-which(state==1)\n",
    "  \n",
    "  if(  re_index==100){\n",
    "    reward<- 5# episode end\n",
    "    done<-T\n",
    "  }\n",
    "  else if(re_index==12 |re_index==42|re_index==44|re_index==45    |\n",
    "          re_index==68|re_index==72|re_index==80){\n",
    "    reward<- -2\n",
    "    done<-F\n",
    "  }else{\n",
    "    reward <- -1\n",
    "    done<-F\n",
    "  }\n",
    "  if(re_index==which(current_state==1)){\n",
    "    reward<-reward*2\n",
    "  }\n",
    "  xx<-ceiling(re_index/ 10) ## row\n",
    "  yy<-re_index %% 10  ## col\n",
    "  yy<-ifelse(yy ==0,10,yy)\n",
    "  reward_weight<-sqrt(162)-sqrt((yy-10)^2+(xx-10)^2) #weigthed reward by distance from current state to goal\n",
    "  reward<-reward+reward_weight*0.05\n",
    "  c(reward,done)\n",
    "  \n",
    "}\n",
    "action<-c(\"left\",\"right\",\"down\",\"up\")\n",
    "\n",
    "state_size <-ncol(stm)*nrow(stm)\n",
    "\n",
    "\n",
    "epoch<-50\n",
    "mini_batch<-20\n",
    "init_data<-c(1,rep(0,state_size-1))\n",
    "dis_f<-0.99\n",
    "reward_list<-c()\n",
    "final_action_list<-list()\n",
    "step_list<-c()\n",
    "q_table<-list()\n",
    "replay_buffer<-list()\n",
    "bi<-1\n",
    "\n",
    "# target Network <- DQN Model\n",
    "set_weights(target_qn,get_weights(model))\n",
    "\n",
    "for(i in 1:20000){\n",
    "  total_r<-0 ## total reward\n",
    "  episode_done<-0\n",
    "  \n",
    "  \n",
    "  \n",
    "  step<-1\n",
    "  action_list<-NULL\n",
    "  st<-c(1,1)\n",
    "  \n",
    "  \n",
    "  while(episode_done==0){\n",
    "    \n",
    "    if(step >1){\n",
    "\n",
    "      qvalue<-predict(model,t(next_state))\n",
    "      action_index<-which.max(qvalue)\n",
    "      current_state<-next_state\n",
    "      \n",
    "    }else{\n",
    "      qvalue<-predict(model,t(init_data))\n",
    "      current_state<-init_data\n",
    "      action_index<-which.max(qvalue)\n",
    "      \n",
    "    }\n",
    "    \n",
    "    th<-1/(i/50+10)\n",
    "    if(runif(1) < th){ ## e-greedy search\n",
    "      next_action<-  action[sample(1:4,1)]\n",
    "      \n",
    "    }else{\n",
    "      next_action<-action[action_index]\n",
    "    }\n",
    "    \n",
    "    \n",
    "    ####### if episode smaller than 10, just choose action randomly\n",
    "    if(i < 10){\n",
    "      next_action<-  action[sample(1:4,1)]\n",
    "    }\n",
    "    \n",
    "    action_list<-c(action_list,next_action)\n",
    "    st<-move(st,next_action)\n",
    "    state_index<-stm[st[1],st[2]]\n",
    "    \n",
    "    \n",
    "    next_state<-next_where(state_index)\n",
    "    re_ep<-return_reward(next_state,current_state) ## get a reward and Whether the episode ends for action(next state)\n",
    "    \n",
    "    total_r<-total_r+re_ep[1]\n",
    "    episode_done<-re_ep[2]\n",
    "    step<-step+1\n",
    "    \n",
    "    \n",
    "    \n",
    "    #########       \n",
    "    #### store current state, action, reward, done, next_state at replay_buffer\n",
    "    \n",
    "    replay_buffer[[bi]]<-  c(which(current_state==1),next_action,re_ep,state_index)\n",
    "    bi<-bi+1\n",
    "    if(bi == 100000){\n",
    "      bi <- 1\n",
    "    }\n",
    "    \n",
    "    \n",
    "    if(step == 500){\n",
    "      cat(\"\\n\",i,\" epsode-\",step) \n",
    "      step_list<-c(step_list,step)\n",
    "      final_action_list[[i]]<-action_list\n",
    "      reward_list<-c(reward_list,total_r)\n",
    "      \n",
    "      cat(\"\\n final location\")\n",
    "      print(coord(next_state))\n",
    "      ts.plot(reward_list,main=paste0((reward_list)[length(reward_list)],\"-\",step,\"-\",min(step_list)))\n",
    "   \n",
    "      break;\n",
    "    }\n",
    "    \n",
    "    if(episode_done==1){\n",
    "      \n",
    "      cat(\"\\n\",i,\" epsode-\",step) \n",
    "      cat(\"\\n final location\")\n",
    "      print(coord(next_state))\n",
    "      step_list<-c(step_list,step)\n",
    "      final_action_list[[i]]<-action_list\n",
    "      reward_list<-c(reward_list,total_r)\n",
    "      ts.plot(reward_list,main=paste0((reward_list)[length(reward_list)],\"-\",step,\"-\",min(step_list)))\n",
    "      break;\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  if(i > 9){\n",
    "    \n",
    "    \n",
    "    ## it learns once in five times of episode\n",
    "    \n",
    "    if(i %% 5==0){\n",
    "      \n",
    "      \n",
    "      ### sampling from replay_buffer\n",
    "      for(u in 1:20){\n",
    "        \n",
    "        sam<-sample(1:length(replay_buffer),mini_batch)\n",
    "        sam_1<-replay_buffer[sam]\n",
    "        \n",
    "        x_stack<-NULL\n",
    "        y_stack<-NULL\n",
    "        q<-1\n",
    "        \n",
    "        for(q in 1:length(sam_1)){\n",
    "          re<-rep(0,state_size)\n",
    "          re[as.numeric(sam_1[[q]][1])]<-1\n",
    "          x_stack<- rbind(x_stack,re) ##x stack\n",
    "          \n",
    "          qvalue<-predict(model,t(re))\n",
    "          \n",
    "          ######### state, action, reward, done, next_state\n",
    "          ## sam_1[[q]][1] current_state\n",
    "          ## sam_1[[q]][2] action\n",
    "          ## sam_1[[q]][3] reward\n",
    "          ## sam_1[[q]][4] episode done\n",
    "          ## sam_1[[q]][5] next_state\n",
    "          \n",
    "          if(      sam_1[[q]][4]==1){\n",
    "            \n",
    "            qvalue[action==sam_1[[q]][2]]<-as.numeric(sam_1[[q]][3])\n",
    "            y_stack<-rbind(y_stack,qvalue) ## y stack\n",
    "            \n",
    "          }else{\n",
    "            \n",
    "            re2<-rep(0,state_size)\n",
    "            re2[as.numeric(sam_1[[q]][5])]<-1\n",
    "        \n",
    "           ## feed forward using target netwrok\n",
    "            true_y<- max(predict(target_qn,t(re2)))\n",
    "            qvalue[action==sam_1[[q]][2]]<-   as.numeric(sam_1[[q]][3])+dis_f*true_y\n",
    "            y_stack<-rbind(y_stack,qvalue) ## y stack\n",
    "            \n",
    "          }\n",
    "          \n",
    "        }\n",
    "        \n",
    "        model %>% fit(\n",
    "          x_stack, y_stack,\n",
    "            batch_size = 10,\n",
    "            epochs = 1,\n",
    "            verbose = 0\n",
    "          )\n",
    "        \n",
    "        \n",
    "      }\n",
    "      cat(\"\\n\",\"DQN update\")\n",
    "  \n",
    "      ###### copy qnetwork to target network\n",
    "      # target_qn<-model\n",
    "     # predict(target_qn,t(re2))\n",
    "     # predict(model,t(re2))\n",
    "    \n",
    "     set_weights(target_qn,get_weights(model))\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
